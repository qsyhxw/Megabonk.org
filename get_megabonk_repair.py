import json
import time
import re
from datetime import datetime
from playwright.sync_api import sync_playwright

# === é…ç½® ===
TARGET_COUNT = 150

# === è¾…åŠ©å‡½æ•° ===
def parse_score(score_str):
    try:
        s = score_str.lower().strip()
        if 'm' in s:
            return int(float(s.replace('m', '')) * 1000000)
        elif 'k' in s:
            return int(float(s.replace('k', '')) * 1000)
        else:
            return int(s.replace(',', ''))
    except:
        return 0

def extract_name_from_url(url):
    if not url: return ""
    clean_url = url.split('?')[0]
    filename = clean_url.split('/')[-1]
    name = filename.split('.')[0]
    return name

def process_single_rank(page, rank, is_retry=False):
    """
    å°è£…å¥½çš„å•è¡Œé‡‡é›†å‡½æ•°ï¼Œç”¨äºä¸»å¾ªç¯å’Œè¡¥å½•å¾ªç¯
    """
    target_index = rank - 1
    row_selector = f'div[data-index="{target_index}"]'
    row_locator = page.locator(row_selector)
    
    # 1. è‡ªåŠ¨å¯»è·¯
    search_attempts = 0
    while row_locator.count() == 0 and search_attempts < 20:
        # å¦‚æœæ˜¯è¡¥å½•æ¨¡å¼ï¼Œä¸”è¦æ˜¯æ‰¾å‰å‡ åï¼Œå¯èƒ½éœ€è¦å¾€å›æ»š
        if is_retry and rank < 10:
            page.mouse.wheel(0, -500) # å¾€ä¸Šæ»š
        else:
            page.mouse.wheel(0, 300) # å¾€ä¸‹æ»š
            
        time.sleep(0.3)
        search_attempts += 1
    
    if row_locator.count() == 0:
        print(f"âŒ æ— æ³•æ‰¾åˆ°ç¬¬ {rank} å")
        return None

    # 2. ç¡®ä¿è§†é‡ & é˜²é®æŒ¡
    try:
        row_locator.scroll_into_view_if_needed()
        # ã€å…³é”®ä¿®å¤ã€‘å¾€ä¸‹æ»šä¸€ç‚¹ï¼Œå†å¾€ä¸Šå›ä¸€ç‚¹ï¼Œç¡®ä¿ä¸è¢« Header é®æŒ¡
        page.mouse.wheel(0, -150) 
        time.sleep(0.5)
    except:
        pass

    # 3. å±•å¼€é€»è¾‘
    expanded = False
    retry_click = 0
    
    while not expanded and retry_click < 3:
        box = row_locator.bounding_box()
        if not box: break
        
        initial_height = box['height']
        if initial_height > 150:
            expanded = True
            break
        
        # ç‚¹å‡»ç­–ç•¥
        if retry_click == 0:
            # ç‚¹æœ€å³è¾¹
            click_x = box['x'] + box['width'] * 0.95
            click_y = box['y'] + box['height'] / 2
            page.mouse.click(click_x, click_y)
        else:
            # å¼ºåˆ¶ç‚¹ä¸­é—´
            row_locator.click(force=True)
        
        time.sleep(0.8 + retry_click * 0.5) # è¡¥å½•æ—¶å¤šç­‰ä¸€ä¼š
        
        new_box = row_locator.bounding_box()
        if new_box and new_box['height'] > initial_height + 50:
            expanded = True
        else:
            retry_click += 1

    # 4. æ•°æ®æå–
    try:
        imgs = row_locator.locator('img').all()
        items = []
        weapons = []
        tomes = []
        char_name = ""
        country_data = None
        
        for img in imgs:
            src = img.get_attribute('src')
            if not src: continue
            name_id = extract_name_from_url(src)
            
            if "/weapon/" in src:
                if name_id not in weapons: weapons.append(name_id)
            elif "/tome/" in src:
                if name_id not in tomes: tomes.append(name_id)
            elif "/passive/" in src or "/item/" in src:
                if name_id not in items: items.append(name_id)
            elif "/flags/" in src:
                country_data = {"code": name_id, "name": name_id}
            elif "/characters/" in src:
                char_name = name_id
            elif "twitch" not in src and "youtube" not in src and "discord" not in src:
                    if name_id not in items and name_id not in weapons and name_id not in tomes and name_id != char_name:
                        items.append(name_id)

        text = row_locator.inner_text()
        parts = [p.strip() for p in text.replace('\n', '|').split('|') if p.strip()]
        
        score_str = "0"
        player_name = "Unknown"
        for idx, part in enumerate(parts):
            if re.match(r'^\d+(\.\d+)?[mk]?$', part.lower()):
                score_str = part
                if idx + 1 < len(parts): player_name = parts[idx+1]
                break

        links = row_locator.locator('a').all()
        video_url = ""
        for link in links:
            href = link.get_attribute('href')
            if href and ("twitch" in href or "youtu" in href):
                video_url = href
                break

        return {
            "rank": rank,
            "playerName": player_name,
            "kills": parse_score(score_str),
            "character": char_name,
            "country": country_data,
            "weapons": weapons,
            "tomes": tomes,
            "items": items,
            "videoURL": video_url
        }
    except Exception as e:
        print(f"âŒ è§£æå‡ºé”™: {e}")
        return None

def scrape_repair():
    print(f"ğŸš€ å¯åŠ¨å®Œç¾ç‰ˆé‡‡é›† (å«è‡ªåŠ¨è¡¥å½•)...")
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False) 
        page = browser.new_page()
        page.set_viewport_size({"width": 1400, "height": 900})

        print("ğŸŒ è®¿é—® Megabonk...")
        page.goto("https://megabonk.fun", timeout=90000, wait_until="domcontentloaded")
        page.wait_for_selector('div[data-index="0"]', timeout=60000)
        
        collected_data_map = {} # ä½¿ç”¨å­—å…¸ rank -> dataï¼Œæ–¹ä¾¿æ›´æ–°
        
        # === ç¬¬ä¸€é˜¶æ®µï¼šä¸»å¾ªç¯ ===
        print("==== ç¬¬ä¸€é˜¶æ®µï¼šé¡ºåºé‡‡é›† 1-150 ====")
        for rank in range(1, TARGET_COUNT + 1):
            data = process_single_rank(page, rank)
            if data:
                collected_data_map[rank] = data
                status = "âœ…" if len(data['items']) > 0 else "âŒå¾…è¡¥å½•"
                print(f"   {status} #{rank} {data['playerName']} | ç‰©å“: {len(data['items'])}")
            else:
                print(f"   âŒ #{rank} é‡‡é›†å¤±è´¥")

        # === ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½è¡¥å½• ===
        print("\n==== ç¬¬äºŒé˜¶æ®µï¼šæ£€æŸ¥å¹¶è¡¥å½•ç¼ºå¤±æ•°æ® ====")
        
        # æ‰¾å‡ºç‰©å“æ•°ä¸º 0 çš„ Rank
        retry_ranks = []
        for rank in range(1, TARGET_COUNT + 1):
            if rank in collected_data_map:
                if len(collected_data_map[rank]['items']) == 0:
                    retry_ranks.append(rank)
            else:
                retry_ranks.append(rank) # å¦‚æœå®Œå…¨æ²¡æŠ“åˆ°ä¹Ÿè¦è¡¥
        
        if not retry_ranks:
            print("ğŸ‰ å®Œç¾ï¼æ²¡æœ‰éœ€è¦è¡¥å½•çš„æ•°æ®ã€‚")
        else:
            print(f"âš ï¸ å‘ç° {len(retry_ranks)} æ¡æ•°æ®ä¸å®Œæ•´ï¼Œå¼€å§‹è¡¥å½•: {retry_ranks}")
            
            for rank in retry_ranks:
                print(f"   ğŸ”„ æ­£åœ¨è¡¥å½• #{rank} ...")
                
                # è¡¥å½•æ—¶ï¼Œæˆ‘ä»¬å°è¯•å¤šç»™å‡ æ¬¡æœºä¼š
                retry_attempts = 0
                success = False
                while retry_attempts < 2 and not success:
                    new_data = process_single_rank(page, rank, is_retry=True)
                    if new_data and len(new_data['items']) > 0:
                        collected_data_map[rank] = new_data
                        print(f"      âœ… è¡¥å½•æˆåŠŸï¼#{rank} ç‰©å“: {len(new_data['items'])}")
                        success = True
                    else:
                        print(f"      ... å°è¯• {retry_attempts+1} å¤±è´¥ï¼Œé‡è¯•ä¸­")
                        # ç¨å¾®åŠ¨ä¸€ä¸‹é¼ æ ‡ï¼Œæˆ–è€…æ»šä¸€ä¸‹ï¼Œæ”¹å˜ç¯å¢ƒ
                        page.mouse.wheel(0, -100)
                        time.sleep(1)
                        retry_attempts += 1
                
                if not success:
                    print(f"      âŒ è¡¥å½•æ”¾å¼ƒï¼š#{rank} (å¯èƒ½çœŸçš„æ²¡æœ‰ç‰©å“)")

        # === ä¿å­˜ ===
        final_list = sorted(collected_data_map.values(), key=lambda x: x['rank'])
        
        final_output = {
            "count": len(final_list),
            "fetched_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "data": final_list
        }

        with open("leaderboard-data.json", "w", encoding="utf-8") as f:
            json.dump(final_output, f, ensure_ascii=False, indent=4)
            
        print(f"\nğŸ‰ å…¨éƒ¨ç»“æŸï¼æœ€ç»ˆé‡‡é›† {len(final_list)} æ¡ã€‚")
        browser.close()

if __name__ == "__main__":
    scrape_repair()